rule_files:
  - traffic.yaml
evaluation_interval: 1m
tests:
  - interval: 1m
    external_labels:
      site: esams
    input_series:
      - series: 'cluster_layer_code:trafficserver_responses_total:rate5m{cluster="cache_text",layer="tls",site="esams",code="200"}'
        # See https://prometheus.io/docs/prometheus/latest/configuration/unit_testing_rules/
        values: '5000+0x58 3000'
      - series: 'cluster_layer_code:trafficserver_responses_total:rate5m{cluster="cache_text",layer="tls",site="esams",code="404"}'
        values: '42+0x59'
    alert_rule_test:
    - alertname: EdgeTrafficDrop
      eval_time: 60m
      exp_alerts:
       - exp_labels:
           severity: critical
           team: traffic
           site: esams
         exp_annotations:
           summary: '60% request drop in text@esams during the past 30 minutes'
           description: "Critical if requests percentage difference compared to 30 minutes ago is more than 70%"
           dashboard: https://grafana.wikimedia.org/d/000000479/frontend-traffic?viewPanel=12&orgId=1&from=now-24h&to=now&var-site=esams&var-cache_type=text
           runbook: https://wikitech.wikimedia.org/wiki/Monitoring/EdgeTrafficDrop

  - interval: 1m
    input_series:
      - series: 'up{instance="cp3030:9331", job="varnish-text"}'
        values: '1 0 0 0 0 0 0'
    alert_rule_test:
    - alertname: VarnishPrometheusExporterDown
      eval_time: 6m
      exp_alerts:
       - exp_labels:
           severity: warning
           team: traffic
           instance: "cp3030:9331"
           job: "varnish-text"
         exp_annotations:
           summary: "Varnish Exporter on instance cp3030:9331 is unreachable"
           description: "Prometheus has been unable to fetch metrics from Varnish Exporter on host cp3030:9331 job(varnish-text) for more than 5 minutes. Make sure the exporter is running on the host."

  - interval: 1m
    external_labels:
      site: eqiad
    input_series:
      # Should trigger a fail
      - series: 'pybal_bgp_session_established{cluster="lvs", instance="lvs1234:9090", job="pybal", local_asn="64600", peer="192.0.2.1"}'
        values: '1 0 0 0 0 0 0'
      - series: 'pybal_bgp_enabled{cluster="lvs", instance="lvs1234:9090", job="pybal"}'
        values: '1 1 1 1 1 1 1'
      # Should pass
      - series: 'pybal_bgp_session_established{cluster="lvs", instance="lvs5678:9090", job="pybal", local_asn="64600", peer="192.0.2.2"}'
        values: '1 1 1 1 1 1 1'
      - series: 'pybal_bgp_enabled{cluster="lvs", instance="lvs5678:9090", job="pybal"}'
        values: '1 1 1 1 1 1 1'
    alert_rule_test:
      - alertname: PyBalBGPUnstable
        eval_time: 6m
        exp_alerts:
          - exp_labels:
              cluster: "lvs"
              job: "pybal"
              instance: "lvs1234:9090"
              local_asn: "64600"
              peer: "192.0.2.1"
              severity: warning
              team: traffic
            exp_annotations:
              summary: "PyBal BGP sessions on instance lvs1234 are failing"
              description: "PyBal BGP session establishment with the peer 192.0.2.1 is unstable on host lvs1234."
              dashboard: "https://grafana.wikimedia.org/d/000000488/pybal-bgp?var-datasource=eqiad prometheus%2Fops&var-server=lvs1234"
