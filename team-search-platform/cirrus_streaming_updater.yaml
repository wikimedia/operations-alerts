##
# flink alerts for the Cirrus Streaming Updater
# deploy-tag: k8s
groups:
  - name: cirrus-streaming-updater
    rules:
## Cirrus Streaming Updater Producer alerts
      - alert: CirrusStreamingUpdaterFlinkJobUnstable
        # We try to detect crashloop, use the uptime of the job and take its max over a 10minute period.
        # Using resets() or repetive instant checks for 10m do not seem to work because:
        # - the series might differ if the k8s pod crashes making resets unable to detect the resets of the counter
        # - the series might disappear while the job is being restarted making instant checks flappy
        # max_over_time does seem to workaround these problems and will only fail if the series goes away for more than
        # 10 mins in which case the CirrusStreamingUpdaterFlinkJobNotRunning alert should have been triggered a long time ago
        expr: |
            max by (job_name, release, kubernetes_namespace) (max_over_time(flink_jobmanager_job_uptime{kubernetes_namespace="cirrus-streaming-updater", job_name !~ ".*_backfill_.*"}[10m])) < 300000
        for: 1m
        labels:
          team: search-platform
          severity: critical
        annotations:
          summary: "{{$labels.job_name}} in {{$externalLabels.site}} ({{$externalLabels.prometheus}}) is unstable"
          description: "The flink job has an uptime below 5 minutes for the past 10 minutes, it is probably trying to start in vain"
          runbook: https://wikitech.wikimedia.org/wiki/Search#Streaming_Updater
          dashboard: https://grafana.wikimedia.org/d/K9x0c4aVk/flink-app?var-datasource={{$externalLabels.site}}+prometheus%2F{{$externalLabels.prometheus}}&var-namespace=cirrus-streaming-updater&var-helm_release={{$labels.release}}

      - alert: CirrusProducerFlinkJobNotRunning
        expr: |
          absent(flink_jobmanager_job_uptime{kubernetes_namespace="cirrus-streaming-updater", job_name=~"cirrus_streaming_updater_producer_(eqiad|codfw)"})
        for: 5m
        labels:
          team: search-platform
          severity: critical
          job_name:
        annotations:
          summary: "cirrus_streaming_updater_producer in {{$externalLabels.site}} ({{$externalLabels.prometheus}}) is not running"
          description: "The flink job is not running"
          runbook: https://wikitech.wikimedia.org/wiki/Search#Streaming_Updater
          dashboard: https://grafana.wikimedia.org/d/K9x0c4aVk/flink-app?var-datasource={{$externalLabels.site}}+prometheus%2F{{$externalLabels.prometheus}}&var-namespace=cirrus-streaming-updater&var-helm_release=producer

      - alert: CirrusStreamingUpdaterFlinkNoRegisteredTask
        expr: |
           flink_jobmanager_numRegisteredTaskManagers{kubernetes_namespace="cirrus-streaming-updater"} < 1
        for: 5m
        labels:
          team: search-platform
          severity: critical
        annotations:
          summary: "cirrus-streaming-updater job {{ $labels.job_name }} in {{$externalLabels.site}} ({{$externalLabels.prometheus}}) is running without any taskmanagers"
          description: "The flink job is running but without any taskmanagers (probably doing nothing)"
          runbook: https://wikitech.wikimedia.org/wiki/Search#Streaming_Updater
          dashboard: https://grafana.wikimedia.org/d/K9x0c4aVk/flink-app?var-datasource={{$externalLabels.site}}+prometheus%2F{{$externalLabels.prometheus}}&var-namespace=cirrus-streaming-updater&var-helm_release={{$labels.release}}

      - alert: CirrusConsumerFetchErrorRate
        # flink_taskmanager_job_task_operator_numRecordsOut is reported as a
        # gauge, though rate() should be applied to counters only
        # pint disable promql/rate
        expr: |
            sum(rate(flink_taskmanager_job_task_operator_numRecordsOut{job_name=~"cirrus_streaming_updater_consumer_.*", kubernetes_namespace="cirrus-streaming-updater", release !~ ".*backfill", operator_name="fetch_failure_sink:_Writer"}[5m])) by (job_name, kubernetes_namespace, release) > 0.1
        for: 20m
        labels:
          team: search-platform
          severity: critical
        annotations:
          summary: "{{$labels.job_name}} in {{$externalLabels.site}} ({{$externalLabels.prometheus}}): fetch error rate too high"
          description: "The flink job is failing to fetch the cirrus documents at a high rate"
          runbook: https://wikitech.wikimedia.org/wiki/Search#Streaming_Updater
          dashboard: https://grafana.wikimedia.org/d/K9x0c4aVk/flink-app?var-datasource={{$externalLabels.site}}+prometheus%2F{{$externalLabels.prometheus}}&var-namespace=cirrus-streaming-updater&var-helm_release={{$labels.release}}

      - alert: CirrusConsumerRerenderFetchErrorRate
        # flink_taskmanager_job_task_operator_numRecordsOut is reported as a
        # gauge, though rate() should be applied to counters only
        # pint disable promql/rate
        expr: |
            sum(rate(flink_taskmanager_job_task_operator_numRecordsOut{job_name=~"cirrus_streaming_updater_consumer_.*", kubernetes_namespace="cirrus-streaming-updater", release !~ ".*backfill", operator_name="rerender_fetch_failure_sink:_Writer"}[5m])) by (job_name, kubernetes_namespace, release) > 0.1
        for: 20m
        labels:
          team: search-platform
          severity: critical
        annotations:
          summary: "{{$labels.job_name}} in {{$externalLabels.site}} ({{$externalLabels.prometheus}}): fetch error (rerenders) rate too high"
          description: "The flink job is failing to fetch the cirrus documents at a high rate for rerenders"
          runbook: https://wikitech.wikimedia.org/wiki/Search#Streaming_Updater
          dashboard: https://grafana.wikimedia.org/d/K9x0c4aVk/flink-app?var-datasource={{$externalLabels.site}}+prometheus%2F{{$externalLabels.prometheus}}&var-namespace=cirrus-streaming-updater&var-helm_release={{$labels.release}}

      - alert: CirrusStreamingUpdaterRateTooLow
        # flink_taskmanager_job_task_operator_bulk_change_type_result_actions
        # is reported as a gauge, though rate() should be applied to counters
        # only
        # pint disable promql/rate
        expr:
          sum by (app) (rate(flink_taskmanager_job_task_operator_bulk_change_type_result_actions[5m])) < 80
        for: 10m
        labels:
          team: search-platform
          severity: critical
        annotations:
          summary: "CirrusSearch update rate from {{$labels.app}} is critically low"
          description: "Updates are either not flowing, or flowing at a greatly reduced rate."
          runbook: https://wikitech.wikimedia.org/wiki/Search#Streaming_Updater
          dashboard: https://grafana.wikimedia.org/d/jKqki4MSk/cirrus-streaming-updater
      # TODO: alert for CirrusConsumerFlinkJobNotRunning once we are running the consumer in eqiad/codfw

      - alert: CirrusStreamingUpdaterSetWeightedTagsTooLow
        expr: |
          sum by (release)(increase(flink_taskmanager_job_task_operator_weighted_tags_tag_prefix_set{job_name=~"cirrus_streaming_updater_consumer_.*", release !~ ".*backfill"}[10m])) < 10
        for: 10m
        labels:
          team: search-platform
          severity: critical
        annotations:
          summary: "CirrusSearch {{$labels.release}}@{{$externalLabels.site}} is setting too few weighted tags"
          description: "The number of weighted_tags set is too low in the last 10 minutes"
          runbook: https://wikitech.wikimedia.org/wiki/Search#Streaming_Updater
          dashboard: https://grafana.wikimedia.org/d/fe251f4f-f6cf-4010-8d78-5f482255b16f/cirrussearch-update-pipeline-weighted-tags?orgId=1&var-tag_prefix=All&var-search_cluster_site={{$externalLabels.site}}&var-search_cluster={{$labels.release}}

      - alert: CirrusStreamingUpdaterClearWeightedTagsTooLow
        expr: |
          sum by (release)(increase(flink_taskmanager_job_task_operator_weighted_tags_tag_prefix_clear{job_name=~"cirrus_streaming_updater_consumer_.*", release !~ ".*backfill"}[10m])) < 10
        for: 10m
        labels:
          team: search-platform
          severity: critical
        annotations:
          summary: "CirrusSearch {{$labels.release}}@{{$externalLabels.site}} is clearing too few weighted tags"
          description: "The number of weighted_tags cleared is too low in the last 10 minutes"
          runbook: https://wikitech.wikimedia.org/wiki/Search#Streaming_Updater
          dashboard: https://grafana.wikimedia.org/d/fe251f4f-f6cf-4010-8d78-5f482255b16f/cirrussearch-update-pipeline-weighted-tags?orgId=1&var-tag_prefix=All&var-search_cluster_site={{$externalLabels.site}}&var-search_cluster={{$labels.release}}
