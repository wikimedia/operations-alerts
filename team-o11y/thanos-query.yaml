# deploy-tag: ops
# deploy-site: eqiad, codfw

# Taken from upstream https://github.com/thanos-io/thanos/blob/main/examples/alerts/alerts.md#query
groups:
  - name: thanos-query
    rules:

# 'code' label is added dynamically as errors come in, thus disable pint
# check for missing series

    - alert: ThanosQueryHttpRequestQueryErrorRateHigh
      annotations:
        description: Thanos Query {{$labels.job}} is failing to handle {{$value | humanize}}%
          of "query" requests.
        summary: Thanos Query is failing to handle requests.
        dashboard: https://grafana.wikimedia.org/d/af36c91291a603f1d9fbdabdd127ac4a/thanos-query
        runbook: https://wikitech.wikimedia.org/wiki/Thanos#Alerts
      # pint disable promql/series
      expr: |
        (
          sum by (job) (rate(http_requests_total{code=~"5..", job=~".*thanos-query.*", handler="query"}[5m]))
        /
          sum by (job) (rate(http_requests_total{job=~".*thanos-query.*", handler="query"}[5m]))
        ) * 100 > 5
      for: 5m
      labels:
        severity: critical
        team: o11y

    - alert: ThanosQueryHttpRequestQueryRangeErrorRateHigh
      annotations:
        description: Thanos Query {{$labels.job}} is failing to handle {{$value | humanize}}%
          of "query_range" requests.
        summary: Thanos Query is failing to handle requests.
        dashboard: https://grafana.wikimedia.org/d/af36c91291a603f1d9fbdabdd127ac4a/thanos-query
        runbook: https://wikitech.wikimedia.org/wiki/Thanos#Alerts
      # pint disable promql/series
      expr: |
        (
          sum by (job) (rate(http_requests_total{code=~"5..", job=~".*thanos-query.*", handler="query_range"}[5m]))
        /
          sum by (job) (rate(http_requests_total{job=~".*thanos-query.*", handler="query_range"}[5m]))
        ) * 100 > 5
      for: 5m
      labels:
        severity: critical
        team: o11y

    - alert: ThanosQueryGrpcServerErrorRate
      annotations:
        description: Thanos Query {{$labels.job}} is failing to handle {{$value | humanize}}%
          of requests.
        summary: Thanos Query is failing to handle requests.
        dashboard: https://grafana.wikimedia.org/d/af36c91291a603f1d9fbdabdd127ac4a/thanos-query
        runbook: https://wikitech.wikimedia.org/wiki/Thanos#Alerts
      expr: |
        (
          sum by (job) (rate(grpc_server_handled_total{grpc_code=~"Unknown|ResourceExhausted|Internal|Unavailable|DataLoss|DeadlineExceeded", job=~".*thanos-query.*"}[5m]))
        /
          sum by (job) (rate(grpc_server_started_total{job=~".*thanos-query.*"}[5m]))
        * 100 > 5
        )
      for: 5m
      labels:
        severity: warning
        team: o11y

    - alert: ThanosQueryGrpcClientErrorRate
      annotations:
        description: Thanos Query {{$labels.job}} is failing to send {{$value | humanize}}%
          of requests.
        summary: Thanos Query is failing to send requests.
        dashboard: https://grafana.wikimedia.org/d/af36c91291a603f1d9fbdabdd127ac4a/thanos-query
        runbook: https://wikitech.wikimedia.org/wiki/Thanos#Alerts
      # Exclude Exemplars queries since we're not supporting those yet, and
      # Grafana issues Exemplars queries.
      expr: |
        (
          sum by (job) (rate(grpc_client_handled_total{grpc_code!="OK", grpc_method!="Exemplars", job=~".*thanos-query.*"}[5m]))
        /
          sum by (job) (rate(grpc_client_started_total{grpc_method!="Exemplars", job=~".*thanos-query.*"}[5m]))
        ) * 100 > 5
      for: 5m
      labels:
        severity: warning
        team: o11y

    - alert: ThanosQueryHighDNSFailures
      annotations:
        description: Thanos Query {{$labels.job}} have {{$value | humanize}}% of failing
          DNS queries for store endpoints.
        summary: Thanos Query is having high number of DNS failures.
        dashboard: https://grafana.wikimedia.org/d/af36c91291a603f1d9fbdabdd127ac4a/thanos-query
        runbook: https://wikitech.wikimedia.org/wiki/Thanos#Alerts
      # pint disable promql/series
      expr: |
        (
          sum by (job) (rate(thanos_query_store_apis_dns_failures_total{job=~".*thanos-query.*"}[5m]))
        /
          sum by (job) (rate(thanos_query_store_apis_dns_lookups_total{job=~".*thanos-query.*"}[5m]))
        ) * 100 > 1
      for: 15m
      labels:
        severity: warning
        team: o11y

    - alert: ThanosQueryInstantLatencyHigh
      annotations:
        description: Thanos Query {{$labels.job}} has a 99th percentile latency of {{$value}}
          seconds for instant queries.
        summary: Thanos Query has high latency for queries.
        dashboard: https://grafana.wikimedia.org/d/af36c91291a603f1d9fbdabdd127ac4a/thanos-query
        runbook: https://wikitech.wikimedia.org/wiki/Thanos#Alerts
      expr: |
        (
          histogram_quantile(0.99, sum by (job, le) (rate(http_request_duration_seconds_bucket{job=~".*thanos-query.*", handler="query"}[5m]))) > 40
        and
          sum by (job) (rate(http_request_duration_seconds_bucket{job=~".*thanos-query.*", handler="query"}[5m])) > 0
        )
      for: 10m
      labels:
        severity: critical
        team: o11y

    - alert: ThanosQueryRangeLatencyHigh
      annotations:
        description: Thanos Query {{$labels.job}} has a 99th percentile latency of {{$value}}
          seconds for range queries.
        summary: Thanos Query has high latency for queries.
        dashboard: https://grafana.wikimedia.org/d/af36c91291a603f1d9fbdabdd127ac4a/thanos-query
        runbook: https://wikitech.wikimedia.org/wiki/Thanos#Alerts
      expr: |
        (
          histogram_quantile(0.99, sum by (job, le) (rate(http_request_duration_seconds_bucket{job=~".*thanos-query.*", handler="query_range"}[5m]))) > 90
        and
          sum by (job) (rate(http_request_duration_seconds_count{job=~".*thanos-query.*", handler="query_range"}[5m])) > 0
        )
      for: 10m
      labels:
        severity: critical
        team: o11y


  # Include a rules group for query-frontend as well
  - name: query-frontend
    rules:

    - alert: ThanosQueryInstantLatencyHigh
      annotations:
        description: Thanos Query Frontend {{$labels.job}} has a 99th percentile latency of {{$value}}
          seconds for instant queries.
        summary: Thanos Query Frontend has high latency for queries.
        dashboard: https://grafana.wikimedia.org/d/aa7Rx0oMk/thanos-query-frontend
        runbook: https://wikitech.wikimedia.org/wiki/Thanos#Alerts
      expr: |
        (
          histogram_quantile(0.99, sum by (job, le) (rate(http_request_duration_seconds_bucket{job=~".*thanos-query.*", handler="query-frontend"}[5m]))) > 40
        and
          sum by (job) (rate(http_request_duration_seconds_bucket{job=~".*thanos-query.*", handler="query-frontend"}[5m])) > 0
        )
      for: 10m
      labels:
        severity: critical
        team: o11y

    - alert: ThanosQueryHttpRequestQueryErrorRateHigh
      annotations:
        description: Thanos Query Frontend {{$labels.job}} is failing to handle {{$value | humanize}}%
          of "query-frontend" requests.
        summary: Thanos Query Frontend is failing to handle requests.
        dashboard: https://grafana.wikimedia.org/d/aa7Rx0oMk/thanos-query-frontend
        runbook: https://wikitech.wikimedia.org/wiki/Thanos#Alerts
      # pint disable promql/series
      expr: |
        (
          sum by (job) (rate(http_requests_total{code=~"5..", job=~".*thanos-query.*", handler="query-frontend"}[5m]))
        /
          sum by (job) (rate(http_requests_total{job=~".*thanos-query.*", handler="query-frontend"}[5m]))
        ) * 100 > 5
      for: 5m
      labels:
        severity: critical
        team: o11y
