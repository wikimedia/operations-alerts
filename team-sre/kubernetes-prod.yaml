# deploy-tag: k8s,k8s-mlserve
groups:
- name: kubernetes
  rules:
  - alert: KubernetesContainerOomKilled
    # Alert on containers being OOM killed at least twice within the last 10 minutes
    # Using last_terminated_exitcode == 137 here instead of last_terminated_reason{reason="OOMKilled"} as the latter does not seem to catch all OOM kills properly
    expr: (kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 10m >= 2) and min_over_time(kube_pod_container_status_last_terminated_exitcode[10m]) == 137
    labels:
      team: sre
      severity: warning
    annotations:
      summary: "Container {{ $labels.container }} has been OOM killed at least twice in 10min"
      description: "Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} has been OOMKilled {{ $value }} times in the last 10 minutes"
      runbook: "https://wikitech.wikimedia.org/wiki/Kubernetes" # This needs a proper runbook
      dashboard: https://grafana.wikimedia.org/d/hyl18XgMk?var-site={{ $externalLabels.site }}&var-cluster={{ $externalLabels.prometheus }}&var-namespace={{ $labels.namespace }}&var-pod={{ $labels.pod }}

  - alert: KubernetesContainerReachingMemoryLimit
    expr: (container_memory_working_set_bytes{container!~"POD|"} / (container_spec_memory_limit_bytes{container!~"POD|"} > 0)) > 0.95
    for: 10m
    labels:
      team: sre
      severity: warning
    annotations:
      summary: "Container {{ $labels.pod }}:{{ $labels.container }} is consistently using {{ $value | humanizePercentage }} of its memory limit"
      description: "Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} is consistently using {{ $value | humanizePercentage }} of its memory limit over the last 10min"
      runbook: "https://wikitech.wikimedia.org/wiki/Kubernetes" # This needs a proper runbook
      dashboard: https://grafana.wikimedia.org/d/hyl18XgMk?var-site={{ $externalLabels.site }}&var-cluster={{ $externalLabels.prometheus }}&var-namespace={{ $labels.namespace }}&var-pod={{ $labels.pod }}

  - alert: KubernetesDeploymentUnavailableReplicas
    expr: sum(kube_deployment_status_replicas_unavailable) by (namespace, deployment) > 0
    for: 30m
    labels:
      team: sre
      severity: warning
    annotations:
      summary: "Deployment {{ $labels.deployment }} in {{ $labels.namespace }} at {{ $externalLabels.site }} has persistently unavailable replicas"
      description: "Deployment {{ $labels.deployment }} in namespace {{ $labels.namespace }} at site {{ $externalLabels.site }} has had non-zero unavailable replicas (current: {{ $value }}) for at least 30m"
      runbook: "https://wikitech.wikimedia.org/wiki/Kubernetes/Troubleshooting#Troubleshooting_a_deployment"
      dashboard: https://grafana.wikimedia.org/d/a260da06-259a-4ee4-9540-5cab01a246c8/kubernetes-deployment-details?var-site={{ $externalLabels.site }}&var-cluster={{ $externalLabels.prometheus }}&var-namespace={{ $labels.namespace }}&var-deployment={{ $labels.deployment }}

- name: certmanager
  rules:
  - &CertManagerCertExpirySoon
    alert: CertManagerCertExpirySoon
    expr: certmanager_certificate_expiration_timestamp_seconds - time() < (9 * 24 * 3600)
    for: 5m
    labels:
      team: sre
      severity: warning
    annotations:
      summary: "Certificate {{ $labels.namespace }}/{{ $labels.name }} in is about to expire ({{ $externalLabels.prometheus }}@{{ $externalLabels.site }})"
      description: "The certificate {{ $labels.name }} in namespace {{ $labels.namespace }} is {{ $value | humanizeDuration }} from expiry ({{ $externalLabels.prometheus }}@{{ $externalLabels.site }}). It should have been refreshed 9 days before expiry"
      runbook: https://wikitech.wikimedia.org/wiki/Kubernetes/cert-manager
      dashboard: https://grafana.wikimedia.org/d/vo5tiJTnz?var-site={{ $externalLabels.site }}&var-cluster={{ $externalLabels.prometheus }}&var-namespace={{ $labels.namespace }}

  - <<: *CertManagerCertExpirySoon
    expr: certmanager_certificate_expiration_timestamp_seconds - time() < (7 * 24 * 3600)
    labels:
      team: sre
      severity: critical

- name: calico
  rules:
  - alert: CalicoTyphaDown
    expr: sum(up{kubernetes_namespace="kube-system", k8s_app="calico-typha"}) < 2
    for: 5m
    labels:
      team: sre
      severity: critical
    annotations:
      summary: "Too few ({{ $value }}) calico-typha replicas running"
      description: "There should be at least 2 calico-typha pods up and running in the kube-system namespace"
      runbook: https://wikitech.wikimedia.org/wiki/Calico#Typha"
      dashboard: TODO
