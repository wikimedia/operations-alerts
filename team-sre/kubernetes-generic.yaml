groups:
- name: kubernetes
  rules:
  - alert: KubernetesCalicoDown
    expr: up{job="calico-felix"} == 0
    for: 5m
    labels:
      team: sre
      severity: critical
    annotations:
      summary: "{{ $labels.instance }} is not running calico-node Pod"
      description: "Prometheus has been unable to scrape metrics from calico on host {{ $labels.instance }} job({{ $labels.job }}) for more than 5 minutes. Make sure the calico-node Pod is running on that node."
      runbook: https://wikitech.wikimedia.org/wiki/Calico#Operations
      dashboard: https://grafana.wikimedia.org/d/G8zPL7-Wz/?var-dc={{ $externalLabels.site }}%20prometheus%2F{{ $externalLabels.prometheus }}&var-instance={{ reReplaceAll "([^:]+).*" "$1" $labels.instance }}

  - alert: KubernetesRsyslogDown
    expr: increase(rsyslog_kubernetes_record_seen_total[5m]) == 0
    for: 5m
    labels:
      team: sre
      severity: critical
    annotations:
      summary: "rsyslog on {{ $labels.instance }} is missing kubernetes logs"
      description: "Rsyslog mmkubernetes plugin on {{ $labels.instance }} did not see any records in the last 5 minutes. This might mean that the plugin was terminated."
      runbook: https://wikitech.wikimedia.org/wiki/Kubernetes/Logging#Common_issues
      dashboard: https://grafana.wikimedia.org/d/OagQjQmnk?var-server={{ reReplaceAll "([^.:]+)([.:].*)?" "$1" $labels.instance }}

  - alert: KubeletOperationalLatency
    expr: histogram_quantile(0.99, rate(kubelet_runtime_operations_duration_seconds_bucket{job="k8s-node"}[5m])) > 0.6
    for: 5m
    labels:
      team: sre
      severity: warning
    annotations:
      description: "Kubelet {{ $labels.operation_type }} operations on {{ $labels.instance }} take {{ $value | humanizeDuration }} in p99"
      summary: "High latency for kubelet {{ $labels.operation_type }} operation on {{ $labels.instance }}"
      runbook: "https://wikitech.wikimedia.org/wiki/Kubernetes"
      dashboard: "https://grafana.wikimedia.org/d/000000472?var-cluster={{ $externalLabels.site }} prometheus/{{ $externalLabels.prometheus }}"

  # A continuous rate of non 4xx requests may be a pointer to some client/operation constantly failing or doing the wrong thing.
  # So everything non 2xx|0 is defined as "error" here. The special code 0 is currently (k8s 1.16) used for WATCH requests
  # but that might change[1]. For now we treat 0 as if it where 2xx.
  # The threshold of 5.2% is chosen to account for a high rate of 409s currently seen on ml-serve@eqiad and is subject to change.
  # [1] https://github.com/kubernetes/kubernetes/issues/107949
  - alert: KubernetesAPIErrorRate
    expr: sum by (code) (rate(apiserver_request_count{code!~"^2..|^0$", job="k8s-api"}[5m]))
      / ignoring(code) group_left
      sum (rate(apiserver_request_count{job="k8s-api"}[5m])) > 0.052
    for: 5m
    labels:
      team: sre
      severity: warning
    annotations:
      description: "Kubernetes API servers {{ $externalLabels.prometheus }}@{{ $externalLabels.site }} are responding with {{ $value | humanizePercentage }} HTTP {{ $labels.code }}s"
      summary: "High rate of HTTP {{ $labels.code }} responses from Kubernetes API {{ $externalLabels.prometheus }}@{{ $externalLabels.site }}"
      runbook: "https://wikitech.wikimedia.org/wiki/Kubernetes"
      dashboard: "https://grafana.wikimedia.org/d/000000435?var-site={{ $externalLabels.site }}&var-cluster={{ $externalLabels.prometheus }}"

  # This alert excludes secrets. LIST calls for all secrets are done regularly by helm-state-metics
  # and take quite some time because the objects itself are huge (T303184).
  - &KubernetesAPILatency
    alert: KubernetesAPILatency
    expr: histogram_quantile(0.95, sum by (le, verb, resource) (rate(apiserver_request_duration_seconds_bucket{verb!~"^(?:CONNECT|WATCHLIST|WATCH|PROXY)$", resource!="secrets"}[5m]))) > 0.5
    for: 5m
    labels:
      team: sre
      severity: critical
    annotations:
      description: "Kubernetes API {{ $labels.verb }} requests for {{ $labels.resource }} take {{ $value | humanizeDuration }} in p95 on {{ $externalLabels.prometheus }}@{{ $externalLabels.site }}"
      summary: "High Kubernetes API latency ({{ $labels.verb }} {{ $labels.resource }}) on {{ $externalLabels.prometheus }}@{{ $externalLabels.site }}"
      runbook: "https://wikitech.wikimedia.org/wiki/Kubernetes"
      dashboard: "https://grafana.wikimedia.org/d/000000435?var-site={{ $externalLabels.site }}&var-cluster={{ $externalLabels.prometheus }}"
  # This alerts for all verbs excluding LIST for only secret objects, same threshold as above.
  - <<: *KubernetesAPILatency
    expr: histogram_quantile(0.95, sum by (le, verb, resource) (rate(apiserver_request_duration_seconds_bucket{verb!~"^(?:CONNECT|WATCHLIST|WATCH|PROXY|LIST)$", resource="secrets"}[5m]))) > 0.5
  # Tis alerts for LIST for secrets with a quite high threshold.
  - <<: *KubernetesAPILatency
    expr: histogram_quantile(0.95, sum by (le, verb, resource) (rate(apiserver_request_duration_seconds_bucket{verb="LIST", resource="secrets"}[5m]))) > 1.7
    for: 10m

- name: certmanager
  rules:
  - alert: CertManagerCertNotReady
    expr: certmanager_certificate_ready_status{condition!="True"} == 1
    for: 10m
    labels:
      team: sre
      severity: critical
    annotations:
      summary: "Certificate {{ $labels.namespace }}/{{ $labels.name }} is not in a ready state ({{ $externalLabels.prometheus }}@{{ $externalLabels.site }})"
      description: "The certificate {{ $labels.name }} in namespace {{ $labels.namespace }} is in a not ready state for at least 10min ({{ $externalLabels.prometheus }}@{{ $externalLabels.site }})"
      runbook: https://wikitech.wikimedia.org/wiki/Kubernetes/cert-manager
      dashboard: https://grafana.wikimedia.org/d/vo5tiJTnz?var-site={{ $externalLabels.site }}&var-cluster={{ $externalLabels.prometheus }}&var-namespace={{ $labels.namespace }}

- name: helmrelease
  rules:
  - alert: HelmReleaseBadStatus
    expr: helm_release_status{status!="deployed"} == 1
    # Our usual wait time for helm releases is 10 minutes.
    # So it's okay for releases to be in bad state for 10min
    for: 11m
    labels:
      team: sre
      severity: critical
    annotations:
      summary: "Helm release {{ $labels.namespace }}/{{ $labels.name }} on {{ $externalLabels.prometheus }}@{{ $externalLabels.site }} in state {{ $labels.status }}"
      description: "The Helm release {{ $labels.namespace }}/{{ $labels.name }} on Kubernetes cluster {{ $externalLabels.prometheus }}@{{ $externalLabels.site }} is in state {{ $labels.status }} for more than 11min, you might need to roll back"
      runbook: "https://wikitech.wikimedia.org/wiki/Kubernetes/Deployments#Rolling_back_in_an_emergency"
      dashboard: https://grafana.wikimedia.org/d/UT4GtK3nz?var-site={{ $externalLabels.site }}&var-cluster={{ $externalLabels.prometheus }}&var-namespace={{ $labels.namespace }}
