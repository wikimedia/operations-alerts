# deploy-tag: k8s*
# deploy-site: eqiad, codfw

groups:
- name: calico
  rules:
  - alert: KubernetesCalicoDown
    expr: up{job="calico-felix"} == 0
    for: 5m
    labels:
      team: sre
      severity: critical
    annotations:
      summary: "{{ $labels.instance }} is not running calico-node Pod"
      description: "Prometheus has been unable to scrape metrics from calico on host {{ $labels.instance }} job({{ $labels.job }}) for more than 5 minutes. Make sure the calico-node Pod is running on that node."
      runbook: https://wikitech.wikimedia.org/wiki/Calico#Operations
      dashboard: https://grafana.wikimedia.org/d/G8zPL7-Wz/?var-dc={{ $externalLabels.site }}%20prometheus%2F{{ $externalLabels.prometheus }}&var-instance={{ reReplaceAll "([^:]+).*" "$1" $labels.instance }}

  - alert: CalicoKubeControllersDown
    expr: sum(up{kubernetes_namespace="kube-system", k8s_app="calico-kube-controllers"}) == 0
    for: 5m
    labels:
      team: sre
      severity: critical
    annotations:
      summary: "Calico Kubernetes Controllers not running"
      description: "The calico-kube-controllers pod is not running in the {{ $externalLabels.prometheus }}@{{ $externalLabels.site }} kube-system namespace"
      runbook: https://wikitech.wikimedia.org/wiki/Calico#Kube_Controllers"
      dashboard: TODO

  - alert: CalicoHighMemoryUsage
    expr: sum by (pod, container) (container_memory_working_set_bytes{namespace="kube-system", container=~"calico-.*"}) >= 3 * max by (pod, container) (kube_pod_container_resource_requests{namespace="kube-system", container=~"calico-.*", resource="memory"})
    for: 10m
    labels:
      team: sre
      severity: critical
    annotations:
      summary: "Calico container {{ $labels.pod }}:{{ $labels.container }} is consistently using three times its memory request"
      description: "Calico container {{ $labels.pod }}:{{ $labels.container }} is using at least three times its memory request over the last 10 minutes"
      runbook: https://wikitech.wikimedia.org/wiki/Calico#Resource_Usage
      dashboard: https://grafana.wikimedia.org/d/2AfU0X_Mz?var-site={{ $externalLabels.site }}&var-prometheus={{ $externalLabels.prometheus }}&var-container_name={{ $labels.container }}

- name: kubernetes
  rules:
  - &KubeletOperationalLatency
    alert: KubeletOperationalLatency
    expr: histogram_quantile(0.99, rate(kubelet_runtime_operations_duration_seconds_bucket{job="k8s-node",operation_type!="list_images"}[5m])) > 1.0
    for: 15m
    labels:
      team: sre
      severity: warning
    annotations:
      description: "Kubelet {{ $labels.operation_type }} operations on {{ $labels.instance }} take {{ $value | humanizeDuration }} in p99"
      summary: "High latency for kubelet {{ $labels.operation_type }} operation on {{ $labels.instance }}"
      runbook: "https://wikitech.wikimedia.org/wiki/Kubernetes"
      dashboard: "https://grafana.wikimedia.org/d/000000472?var-cluster={{ $externalLabels.site }}%20prometheus/{{ $externalLabels.prometheus }}"

  - <<: *KubeletOperationalLatency
    expr: histogram_quantile(0.99, rate(kubelet_runtime_operations_duration_seconds_bucket{job="k8s-node", operation_type="list_images"}[5m])) > 1.5

  # A continuous rate of non 4xx requests may be a pointer to some client/operation constantly failing or doing the wrong thing.
  # So everything non 2xx|0 is defined as "error" here. The special code 0 is currently (k8s 1.16) used for WATCH requests
  # but that might change[1]. For now we treat 0 as if it where 2xx.
  # The threshold of 5.2% is chosen to account for a high rate of 409s currently seen on ml-serve@eqiad and is subject to change.
  # [1] https://github.com/kubernetes/kubernetes/issues/107949
  - alert: KubernetesAPIErrorRate
    expr: |
      (sum by (code) (rate(apiserver_request_total{code!~"^2..|^0$", job="k8s-api"}[5m]))
      / ignoring(code) group_left
      sum (rate(apiserver_request_total{job="k8s-api"}[5m])) > 0.052)
    for: 5m
    labels:
      team: sre
      severity: warning
    annotations:
      description: "Kubernetes API servers {{ $externalLabels.prometheus }}@{{ $externalLabels.site }} are responding with {{ $value | humanizePercentage }} HTTP {{ $labels.code }}s"
      summary: "High rate of HTTP {{ $labels.code }} responses from Kubernetes API {{ $externalLabels.prometheus }}@{{ $externalLabels.site }}"
      runbook: "https://wikitech.wikimedia.org/wiki/Kubernetes"
      dashboard: "https://grafana.wikimedia.org/d/000000435?var-site={{ $externalLabels.site }}&var-cluster={{ $externalLabels.prometheus }}"

  - alert: KubernetesAPILatency
    expr: histogram_quantile(0.95, sum by (le, verb, resource) (rate(apiserver_request_duration_seconds_bucket{verb!~"(?:CONNECT|WATCHLIST|WATCH|PROXY)"}[5m]))) > 0.5
    for: 15m
    labels:
      team: sre
      severity: critical
    annotations:
      description: "Kubernetes API {{ $labels.verb }} requests for {{ $labels.resource }} take {{ $value | humanizeDuration }} in p95 on {{ $externalLabels.prometheus }}@{{ $externalLabels.site }}"
      summary: "High Kubernetes API latency ({{ $labels.verb }} {{ $labels.resource }}) on {{ $externalLabels.prometheus }}@{{ $externalLabels.site }}"
      runbook: "https://wikitech.wikimedia.org/wiki/Kubernetes"
      dashboard: "https://grafana.wikimedia.org/d/ddNd-sLnk/kubernetes-api-details?var-site={{ $externalLabels.site }}&var-cluster={{ $externalLabels.prometheus }}&var-latency_percentile=0.95&var-verb={{ $labels.verb }}"

  - alert: KubernetesWorkerUnschedulable
    expr: sum by (node) (kube_node_spec_unschedulable) > 0
    for: 24h
    labels:
      team: sre
      severity: warning
    annotations:
      description: "Kubernetes worker {{ $labels.node }} has been unschedulable for >= 24h {{ $externalLabels.prometheus }}@{{ $externalLabels.site }}"
      summary: "Kubernetes worker {{ $labels.node }} has been unschedulable for >= 24h {{ $externalLabels.prometheus }}@{{ $externalLabels.site }}"
      runbook: "https://wikitech.wikimedia.org/wiki/Kubernetes"
      dashboard: "https://grafana.wikimedia.org/d/G8zPL7-Wz/kubernetes-node?orgId=1&var-dc={{ $externalLabels.site }}%20prometheus%2F{{ $externalLabels.prometheus }}&var-instance={{ $labels.node }}"

- name: certmanager
  rules:
  - alert: CertManagerCertNotReady
    expr: certmanager_certificate_ready_status{condition!="True"} == 1
    for: 10m
    labels:
      team: sre
      severity: critical
    annotations:
      summary: "Certificate {{ $labels.namespace }}/{{ $labels.name }} is not in a ready state ({{ $externalLabels.prometheus }}@{{ $externalLabels.site }})"
      description: "The certificate {{ $labels.name }} in namespace {{ $labels.namespace }} is in a not ready state for at least 10min ({{ $externalLabels.prometheus }}@{{ $externalLabels.site }})"
      runbook: https://wikitech.wikimedia.org/wiki/Kubernetes/cert-manager
      dashboard: https://grafana.wikimedia.org/d/vo5tiJTnz?var-site={{ $externalLabels.site }}&var-cluster={{ $externalLabels.prometheus }}&var-namespace={{ $labels.namespace }}

- name: helmrelease
  rules:
  - alert: HelmReleaseBadStatus
    expr: helm_release_status{status!="deployed"} == 1
    # Our usual wait time for helm releases is 10 minutes.
    # So it's okay for releases to be in bad state for 10min
    for: 11m
    labels:
      team: sre
      severity: critical
    annotations:
      summary: "Helm release {{ $labels.namespace }}/{{ $labels.name }} on {{ $externalLabels.prometheus }}@{{ $externalLabels.site }} in state {{ $labels.status }}"
      description: "The Helm release {{ $labels.namespace }}/{{ $labels.name }} on Kubernetes cluster {{ $externalLabels.prometheus }}@{{ $externalLabels.site }} is in state {{ $labels.status }} for more than 11min, you might need to roll back"
      runbook: "https://wikitech.wikimedia.org/wiki/Kubernetes/Deployments#Rolling_back_in_an_emergency"
      dashboard: https://grafana.wikimedia.org/d/UT4GtK3nz?var-site={{ $externalLabels.site }}&var-cluster={{ $externalLabels.prometheus }}&var-namespace={{ $labels.namespace }}
