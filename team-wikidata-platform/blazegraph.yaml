# deploy-tag: ops
# deploy-site: eqiad, codfw

groups:
  - name: blazegraph
    rules:
      - alert: BlazegraphFreeAllocatorsDecreasingRapidly
        expr: deriv(blazegraph_free_allocators{instance=~".*:919[35]"}[10m]) < -0.03
        for: 5m
        labels:
          team: wikidata-platform
          severity: critical
        annotations:
          summary: "Blazegraph instance {{$labels.instance}} is burning free allocators at a very high rate"
          description: "Blazegraph is misbehaving and will rapidly corrupt its journal."
          runbook: https://wikitech.wikimedia.org/wiki/Wikidata_Query_Service/Runbook#Free_allocators_decrease_rapidly
          dashboard: https://grafana.wikimedia.org/d/000000489/wikidata-query-service

      - alert: BlazegraphFreeAllocatorsRunningOut
        expr: blazegraph_free_allocators < 100000
        for: 1m
        labels:
          team: wikidata-platform
          severity: critical
        annotations:
          summary: "Blazegraph instance {{$labels.instance}} is running out of free allocators"
          description: "Blazegraph will soon be unable to apply updates"
          runbook: https://wikitech.wikimedia.org/wiki/Wikidata_Query_Service/Runbook
          dashboard: https://grafana.wikimedia.org/d/000000489/wikidata-query-service

      - alert: ElevatedMaxLagWDQS
        expr: >-
          max(
            # time since last update in (in seconds since epoch), we want this to be < 10 minutes
            time() -
            label_replace(
              # time of the latest update (in seconds since epoch)
              blazegraph_lastupdated{cluster=~"wdqs-.*", instance=~".*:9193"},
              "host", "$1", "instance", "^([^:]+):.*"
            )
            # filter the above vectors to remove nodes which see less than 0.2 queries per second (12 queries per minute), those are probably depooled
            and on(host) label_replace(
              rate(
                org_wikidata_query_rdf_blazegraph_filters_QueryEventSenderFilter_event_sender_filter_StartedQueries{
                  cluster=~"wdqs-(main|scholarly)",
                  instance  =~ ".*:9102"
                }[5m]
              ) > 0.2,
              "host", "$1", "instance", "^([^:]+):.*"
            )
          )/60 > 10
        for: 30m
        labels:
          team: wikidata-platform
          severity: critical
        annotations:
          summary: "WDQS lag is above 10 minutes"
          description: "WDQS cluster in {{ $externalLabels.site }} is throttling all connections due to high lag"
          runbook: https://wikitech.wikimedia.org/wiki/Wikidata_Query_Service/Runbook
          dashboard: https://grafana.wikimedia.org/d/000000489/wikidata-query-service

      - alert: CategoriesQueryServiceUpdateLagTooHigh
        expr: (time() - blazegraph_lastupdated{instance=~".*:9194"})/60 > 2880
        for: 1m
        labels:
          team: wikidata-platform
          severity: critical
        annotations:
          summary: "Categories Query service lag is above 2 days"
          description: "Categories Query service lag on {{$labels.instance}} is above 2 days"
          runbook: https://wikitech.wikimedia.org/wiki/Wikidata_Query_Service/Runbook
          dashboard: https://grafana.wikimedia.org/d/000000489/wikidata-query-service

      - alert: BlazegraphMainTriplesDeltaIncreasing
        expr: |
          (
            (
              max by (cluster) (
                blazegraph_triples{cluster=~"wdqs-.*", instance=~".*:9193"}
              )
              -
              min by (cluster) (
                blazegraph_triples{cluster=~"wdqs-.*", instance=~".*:9193"}
              )
            )
            /
            max by (cluster) (
              blazegraph_triples{cluster=~"wdqs-.*", instance=~".*:9193"}
            )
          ) > 0.02
          and
          increase(
            (
              max by (cluster) (
                blazegraph_triples{cluster=~"wdqs-.*", instance=~".*:9193"}
              )
              -
              min by (cluster) (
                blazegraph_triples{cluster=~"wdqs-.*", instance=~".*:9193"}
              )
            )[30m:1m]
          ) >= 0
        for: 60m
        labels:
          team: wikidata-platform
          severity: warning
        annotations:
          summary: "Large percentage difference in triples count across Blazegraph instances"
          description: |
            The difference between max and min triples count across {{ $labels.cluster }} instances in {{ $externalLabels.site }}
            is {{ $value | humanizePercentage }} of the maximum and is not decreasing, indicating potential replication or sync issues.
            This alert excludes cases where the delta is shrinking (backfill in progress).
          runbook: https://wikitech.wikimedia.org/wiki/Wikidata_Query_Service/Runbook
          dashboard: https://grafana.wikimedia.org/d/000000489/wikidata-query-service

      - alert: BlazegraphCategoriesTriplesDeltaIncreasing
        expr: |
          (
            (
              max by (cluster) (
                blazegraph_triples{cluster=~"wdqs-.*", instance=~".*:9194"}
              )
              -
              min by (cluster) (
                blazegraph_triples{cluster=~"wdqs-.*", instance=~".*:9194"}
              )
            )
            /
            max by (cluster) (
              blazegraph_triples{cluster=~"wdqs-.*", instance=~".*:9194"}
            )
          ) > 0.01
          and
          increase(
            (
              max by (cluster) (
                blazegraph_triples{cluster=~"wdqs-.*", instance=~".*:9194"}
              )
              -
              min by (cluster) (
                blazegraph_triples{cluster=~"wdqs-.*", instance=~".*:9194"}
              )
            )[30m:1m]
          ) >= 0
        for: 60m
        labels:
          team: wikidata-platform
          severity: warning
        annotations:
          summary: "Large percentage difference in categories triples count across Blazegraph instances"
          description: |
            The difference between max and min categories triples count across {{ $labels.cluster }} instances in {{ $externalLabels.site }}
            is {{ $value | humanizePercentage }} of the maximum and is not decreasing, indicating potential replication or sync issues.
            This alert excludes cases where the delta is shrinking (backfill in progress).
          runbook: https://wikitech.wikimedia.org/wiki/Wikidata_Query_Service/Runbook
          dashboard: https://grafana.wikimedia.org/d/000000489/wikidata-query-service
